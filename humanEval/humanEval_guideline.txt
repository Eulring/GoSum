

十分感谢您百忙之中抽空帮助我完成这个标注，整个标注不会占用您太多时间（认真标的话可能 30 min 可以完成）
接下来是这个标注工具的使用方法和须知，请认真阅读

1. 标注的任务是什么？
R1: 标注的任务是用人工判别的方法，来衡量两个summarization模型 （model 1 和 model 2）的输出质量

2. 衡量的标准是什么？
R2: 有两个指标，一个是非冗余性 less-redundancy（希望模型抽取的句子所表达的意思不要重复），另一个是概括性 coverage （希望模型抽取的句子能尽可能的概括文章）

3. 如何进行标注操作？
R3: 由于时间仓促，所以我用 python 书写的这个工具，这个软件的运行只需要 python 和一个 terminal
    step1: 在 terminal 运行 python human_eval.py
    step2: 进行标注，注意标注的输入是 111 / 222 / 333，111 表示模型1在当前表现（冗余性或者概括性）更好，222 表示模型 2 在当前表现更好，333 表示两个模型差不多看不出来区别。。。
    step3: 标注够 50 个以后，系统会自动退出，你后你把保存的结果文件 human_rate.json 微信发我就可以了

4. 关于操作的界面运行脚本后，你会看到:
    （1）最上面显示的是 gold standard
    （2）之后显示的是模型1和模型2抽取句子的交集
    （3）再下面是只有模型1抽取的句子
    （4）最后是只有模型2抽取的句子

5. 标注心得（经验之谈）
    直接看区域（3）和（4）
    非冗余性：观察区域（3）和（2）/ （4）和（2）的句子是否有重复有冗余
    概括性：观察区域（3）/（4）的句子是否更广泛的归纳了（1）或者其他的 point

其他事项
- 为了标注存在倾向性 model 1 和 model 2 的结果是打乱的！
- 你可以在任意时间退出，每标注一次都会保存
- 退出后再进行标注可以读取之前保存的进度
- 显示的没有输入，只有 gold standard 和 prediction，直接拿 prediction 去和 gold standard 之间比较
- 建议把 terminal 拉开到最大！！！要不然看的难受
- 重复输入是为了防止误触
